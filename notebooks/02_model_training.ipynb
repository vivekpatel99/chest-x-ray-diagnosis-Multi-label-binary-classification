{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-02T07:06:22.039762Z",
     "iopub.status.busy": "2025-03-02T07:06:22.039402Z",
     "iopub.status.idle": "2025-03-02T07:06:32.200919Z",
     "shell.execute_reply": "2025-03-02T07:06:32.199836Z",
     "shell.execute_reply.started": "2025-03-02T07:06:22.039734Z"
    }
   },
   "source": [
    "# NIH Chest X-ray Multi label Binary classification using Tensorflow Densenet121 (Transfer learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to project root folder\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:25:58.836148Z",
     "iopub.status.busy": "2025-03-02T10:25:58.835783Z",
     "iopub.status.idle": "2025-03-02T10:26:14.649285Z",
     "shell.execute_reply": "2025-03-02T10:26:14.647955Z",
     "shell.execute_reply.started": "2025-03-02T10:25:58.836120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:14.652600Z",
     "iopub.status.busy": "2025-03-02T10:26:14.651810Z",
     "iopub.status.idle": "2025-03-02T10:26:14.662952Z",
     "shell.execute_reply": "2025-03-02T10:26:14.661875Z",
     "shell.execute_reply.started": "2025-03-02T10:26:14.652529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "found_gpu = tf.config.list_physical_devices('GPU')\n",
    "if not found_gpu:\n",
    "     raise Exception(\"No GPU found\")\n",
    "found_gpu, tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:14.665526Z",
     "iopub.status.busy": "2025-03-02T10:26:14.665122Z",
     "iopub.status.idle": "2025-03-02T10:26:15.927292Z",
     "shell.execute_reply": "2025-03-02T10:26:15.926295Z",
     "shell.execute_reply.started": "2025-03-02T10:26:14.665487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import opendatasets as od\n",
    "\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://www.kaggle.com/datasets/nih-chest-xrays/sample'\n",
    "\n",
    "# Look into the data directory\n",
    "datasets = 'datasets/sample'\n",
    "dataset_path = Path(datasets)\n",
    "IMAGE_DIR = dataset_path /'sample/images'\n",
    "CSV_PATH = dataset_path /'sample/sample_labels.csv'\n",
    "dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "if not dataset_path.is_dir():\n",
    "    od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    print(cfg.DATASET_DIRS.TRAIN_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = cfg.TRAIN.IMG_SIZE\n",
    "BATCH_SIZE = cfg.TRAIN.BATCH_SIZE\n",
    "NUM_EPOCHS = cfg.TRAIN.NUM_EPOCHS\n",
    "LEARNING_RATE = cfg.TRAIN.LEARNING_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_colums = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion',\n",
    "#        'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding',\n",
    "#        'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
    "# 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion',\n",
    "\n",
    "# labels =['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion',\n",
    "#        'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass',\n",
    "#        'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
    "CLASSES_NAME = ['Atelectasis','Effusion','Infiltration', 'Mass']#,'Nodule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader.chest_x_ray_preprocessor import ChestXRayPreprocessor\n",
    "\n",
    "preprocessor = ChestXRayPreprocessor(cfg, labels=CLASSES_NAME)\n",
    "train_ds, valid_ds, pos_weights, neg_weights = preprocessor.get_training_and_validation_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    images, labels = batch\n",
    "    print(images.shape, labels.shape)\n",
    "    print(images[0].shape, images[0].numpy().min(), images[0].numpy().max(), labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_ds.take(1):\n",
    "    images, labels = batch\n",
    "    print(images.shape, labels.shape)\n",
    "    print(images[0].shape, images[0].numpy().min(), images[0].numpy().max(), labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_images(train_ds, num_images=9):\n",
    "  \"\"\"\n",
    "  Plots a random sample of images and their corresponding labels from a TensorFlow dataset.\n",
    "\n",
    "  Args:\n",
    "    train_ds: A TensorFlow dataset object containing image-label pairs.\n",
    "    num_images: The number of images to plot (default: 9).\n",
    "  \"\"\"\n",
    "\n",
    "  # Ensure the dataset is shuffled (if it's not already)\n",
    "  train_ds = train_ds.shuffle(buffer_size=1024)\n",
    "\n",
    "  # Take a batch of images and labels\n",
    "  _images = []\n",
    "  _labels = []\n",
    "  for images, labels in train_ds:\n",
    "      for image, label in zip(images, labels):\n",
    "        _images.append(image)\n",
    "        _labels.append(label)\n",
    "        if len(images) >= num_images:\n",
    "            break\n",
    "        \n",
    "      break\n",
    "\n",
    "  # Create a figure and axes for the plot\n",
    "  plt.figure(figsize=(10, 10))\n",
    "\n",
    "  # Iterate through the images and plot them\n",
    "  for i in range(num_images):\n",
    "    ax = plt.subplot(int(num_images**0.5), int(num_images**0.5), i + 1) # Create a grid of subplots\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray') # Convert to numpy and uint8 for display\n",
    "    plt.title(f\"Label: {labels[i].numpy()}\") # Display the label\n",
    "    plt.axis(\"off\") # Hide the axes\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_images(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Class Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = train_cat_labels_df.shape[0]\n",
    "# positive_frequencies = (train_cat_labels_df==1).sum()/N\n",
    "# negative_frequencies = (train_cat_labels_df==0).sum()/N\n",
    "# positive_frequencies, negative_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.DataFrame(list(positive_frequencies.items()), columns=['class', 'positives'])\n",
    "# data_df['negatives'] = negative_frequencies.values\n",
    "# data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.plot.bar(x='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the above plot, the contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. \n",
    "\n",
    "To have this, we want \n",
    "\n",
    "$$w_{pos} \\times freq_{p} = w_{neg} \\times freq_{n},$$\n",
    "\n",
    "which we can do simply by taking \n",
    "\n",
    "$$w_{pos} = freq_{neg}$$\n",
    "$$w_{neg} = freq_{pos}$$\n",
    "\n",
    "This way, we will be balancing the contribution of positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _pos_weights = negative_frequencies.values\n",
    "# _neg_weights = positive_frequencies.values\n",
    "# positive_frequencies.values\n",
    "\n",
    "# Try adjusting the weight balance slightly\n",
    "# pos_weights = np.sqrt(negative_frequencies.values) * 0.8  # Reduce positive weight slightly\n",
    "# neg_weights = np.sqrt(positive_frequencies.values) * 1.2  # Increase negative weight slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_contirbution = positive_frequencies * _pos_weights\n",
    "# neg_contribution = negative_frequencies * _neg_weights\n",
    "\n",
    "# pos_contirbution, neg_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_data_df = pd.DataFrame(list(pos_contirbution.items()), columns=['class', 'positives'])\n",
    "# weighted_data_df['negatives'] = neg_contribution.values\n",
    "# weighted_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_data_df.plot.bar(x='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted loss calculation to handle class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above figure shows, by applying these weightings the positive and negative labels within each class would have the same aggregate contribution to the loss function. Now let's implement such a loss function. \n",
    "\n",
    "After computing the weights, our final weighted loss for each training case will be \n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y \\log(f(x)) + w_{n}(1-y) \\log( 1 - f(x) ) ).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Return weighted loss function given negative weights and positive weights.\n",
    "\n",
    "    Args:\n",
    "      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n",
    "      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n",
    "    \n",
    "    Returns:\n",
    "      weighted_loss (function): weighted loss function\n",
    "    \"\"\"\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Return weighted loss value. \n",
    "\n",
    "        Args:\n",
    "            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n",
    "            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n",
    "        Returns:\n",
    "            loss (float): overall scalar loss summed across all classes\n",
    "        \"\"\"\n",
    "        # initialize loss to zero\n",
    "        loss = 0.0\n",
    "\n",
    "        for i in range(len(pos_weights)):\n",
    "            y = y_true[:, i]\n",
    "            f_of_x = y_pred[:, i]\n",
    "\n",
    "            f_of_x_log = K.log(f_of_x + epsilon)\n",
    "            f_of_x_1_min_log = K.log((1-f_of_x) + epsilon)\n",
    "\n",
    "            first_term = pos_weights[i] * y * f_of_x_log\n",
    "            sec_term = neg_weights[i] * (1-y) * f_of_x_1_min_log\n",
    "            loss_per_col = - K.mean(first_term + sec_term)\n",
    "            loss += loss_per_col\n",
    "        return loss\n",
    "\n",
    "    return weighted_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare DenseNet121 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "to_monitor = 'val_AUC'\n",
    "mode = 'max'\n",
    "mlflow.start_run()\n",
    "mlflow.tensorflow.autolog(log_models=True, \n",
    "                        log_datasets=False, \n",
    "                        log_input_examples=True,\n",
    "                        log_model_signatures=True,\n",
    "                        keras_model_kwargs={\"save_format\": \"keras\"},\n",
    "                        checkpoint_monitor=to_monitor, \n",
    "                        checkpoint_mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DenseNet121(\n",
    "     include_top=False,\n",
    "     weights=None, #'imagenet', \n",
    "     input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1)  \n",
    ")\n",
    "\n",
    "x = base_model.output\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# and a logistic layer\n",
    "predictions = Dense(len(CLASSES_NAME), activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    'binary_accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='AUC'), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(), \n",
    "               loss=get_weighted_loss(pos_weights, neg_weights),\n",
    "        metrics=METRICS)     \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_POINT_DIR = 'exported_models'\n",
    "checkpoint_prefix = os.path.join(CHECK_POINT_DIR, \"ckpt_{epoch}.keras\")\n",
    "LOG_DIR = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                        save_best_only=True, # Save only the best model based on val_loss\n",
    "                                        monitor=to_monitor,\n",
    "                                        mode=mode,\n",
    "                                        verbose=1),  # Display checkpoint saving messages\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=to_monitor,\n",
    "                                        mode=mode, factor=0.1, patience=5, min_lr=1e-7),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=to_monitor,\n",
    "                                        mode=mode, patience=10, restore_best_weights=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, \n",
    "                    validation_data=valid_ds,\n",
    "                    batch_size=cfg.TRAIN.BATCH_SIZE,\n",
    "                    epochs = cfg.TRAIN.NUM_EPOCHS,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['AUC'])\n",
    "plt.plot(history.history['val_AUC'])\n",
    "plt.legend(['AUC', 'val_AUC'])\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Training AUC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.python.data.ops.dataset_ops import DatasetV2\n",
    "from src.data_loader.chest_x_ray_preprocessor import ChestXRayPreprocessor\n",
    "\n",
    "test_prorcessor  = ChestXRayPreprocessor(cfg, labels=CLASSES_NAME)\n",
    "test_ds = test_prorcessor.get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_ds.take(1):\n",
    "    images, labels = batch\n",
    "    print(images.shape, labels.shape)\n",
    "    print(images[0].shape, images[0].numpy().min(), images[0].numpy().max(), labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(test_ds, return_dict=True, batch_size=BATCH_SIZE)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_metrics(result)\n",
    "mlflow.end_run()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_ds)\n",
    "preds = preds.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4667,
     "sourceId": 7773,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
